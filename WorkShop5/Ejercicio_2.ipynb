{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"11cfaa01c2d489d556e66fcaf9e384c983411f23416837fdb98f5bb9e5af2b30"}},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10153945,"sourceType":"datasetVersion","datasetId":6268895}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"-308Si5318wc","cell_type":"code","source":"pip install datasets evaluate -q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-308Si5318wc","outputId":"75f639d9-08b7-4188-c534-e25a20444743","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:16:50.901850Z","iopub.execute_input":"2024-12-10T03:16:50.902105Z","iopub.status.idle":"2024-12-10T03:17:00.517580Z","shell.execute_reply.started":"2024-12-10T03:16:50.902078Z","shell.execute_reply":"2024-12-10T03:17:00.516424Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"f19ce230","cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorForSeq2Seq, TFAutoModelForSeq2SeqLM, create_optimizer, AdamWeightDecay, pipeline\nfrom datasets import load_dataset\nimport tensorflow as tf\nfrom datasets import Dataset\nimport evaluate\nimport numpy as np\nimport torch\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"id":"f19ce230","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:28:44.797945Z","iopub.execute_input":"2024-12-10T03:28:44.798360Z","iopub.status.idle":"2024-12-10T03:28:44.804151Z","shell.execute_reply.started":"2024-12-10T03:28:44.798330Z","shell.execute_reply":"2024-12-10T03:28:44.803028Z"}},"outputs":[],"execution_count":17},{"id":"4e7c7bb6","cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4e7c7bb6","outputId":"44f2bd0f-a0ab-457a-9f19-be306c5d111f","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:17:38.319422Z","iopub.execute_input":"2024-12-10T03:17:38.319983Z","iopub.status.idle":"2024-12-10T03:17:38.403644Z","shell.execute_reply.started":"2024-12-10T03:17:38.319954Z","shell.execute_reply":"2024-12-10T03:17:38.402794Z"}},"outputs":[{"name":"stdout","text":"Num GPUs Available:  1\n","output_type":"stream"}],"execution_count":3},{"id":"79206908","cell_type":"code","source":"folder_path = r\"/kaggle/input/engtoesp/\"\ndataset_name = \"eng1.csv\"\npath = os.path.join(folder_path, dataset_name)\nprint(path)\ndata = Dataset.from_csv(path)\ndata = data.select(range(15000)) \ndata = data.train_test_split(test_size=0.3)\nprint(data)","metadata":{"id":"79206908","outputId":"a4f1f51e-7180-4fa8-d965-636c49a6e864","colab":{"base_uri":"https://localhost:8080/","height":396},"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:51:31.828541Z","iopub.execute_input":"2024-12-10T03:51:31.828946Z","iopub.status.idle":"2024-12-10T03:51:31.869094Z","shell.execute_reply.started":"2024-12-10T03:51:31.828912Z","shell.execute_reply":"2024-12-10T03:51:31.868270Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/engtoesp/eng1.csv\nDatasetDict({\n    train: Dataset({\n        features: ['engl', 'spa'],\n        num_rows: 10500\n    })\n    test: Dataset({\n        features: ['engl', 'spa'],\n        num_rows: 4500\n    })\n})\n","output_type":"stream"}],"execution_count":80},{"id":"c22f593b-909d-48f5-8c13-890fe063162f","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  #google/flan-t5-small\nmodel = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\") #google/flan-t5-small\n#model = export_and_get_onnx_model('t5-small')\n\nprefix = \"translate: \"\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"engl\"]]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"id":"c22f593b-909d-48f5-8c13-890fe063162f","outputId":"566a7032-4ef5-4a0b-c484-ebd19939993d","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:51:34.668729Z","iopub.execute_input":"2024-12-10T03:51:34.669539Z","iopub.status.idle":"2024-12-10T03:51:35.629417Z","shell.execute_reply.started":"2024-12-10T03:51:34.669504Z","shell.execute_reply":"2024-12-10T03:51:35.628730Z"}},"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n\nAll the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","output_type":"stream"}],"execution_count":81},{"id":"b302007e","cell_type":"code","source":"tokenized_data = data.map(preprocess_function, batched=True, remove_columns=[\"engl\", \"spa\"])","metadata":{"colab":{"referenced_widgets":["27a4362f4e394c11bdd2783291b3b17a","dfa5f5e1685c49b4bae076632457557f"]},"id":"b302007e","outputId":"da233683-9635-4ca9-e2cd-f109f4788f4c","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:51:37.850491Z","iopub.execute_input":"2024-12-10T03:51:37.850858Z","iopub.status.idle":"2024-12-10T03:51:38.464904Z","shell.execute_reply.started":"2024-12-10T03:51:37.850824Z","shell.execute_reply":"2024-12-10T03:51:38.463984Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121d469cf3b6416b84ce0ab74af045ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2c66ef75f841fdaa6f0d7e37738ef9"}},"metadata":{}}],"execution_count":82},{"id":"a560c7e7-3ced-4024-aa6f-09bb51cfd467","cell_type":"code","source":"pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:29:06.898881Z","iopub.execute_input":"2024-12-10T03:29:06.899261Z","iopub.status.idle":"2024-12-10T03:29:15.207565Z","shell.execute_reply.started":"2024-12-10T03:29:06.899228Z","shell.execute_reply":"2024-12-10T03:29:15.206295Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":19},{"id":"f3cd75ab","cell_type":"code","source":"rouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return {k: round(v, 4) for k, v in result.items()}\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\noptimizer = AdamWeightDecay(learning_rate=2e-4, weight_decay_rate=0.01) #2e-5 was before wd was 1e-2, Typically, 1e-4 and 3e-4 work well for most problems","metadata":{"id":"f3cd75ab","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:51:41.416625Z","iopub.execute_input":"2024-12-10T03:51:41.417317Z","iopub.status.idle":"2024-12-10T03:51:41.913753Z","shell.execute_reply.started":"2024-12-10T03:51:41.417278Z","shell.execute_reply":"2024-12-10T03:51:41.912857Z"}},"outputs":[],"execution_count":83},{"id":"076839d6","cell_type":"code","source":"tf_train_set = model.prepare_tf_dataset(\n    tokenized_data[\"train\"],\n    shuffle=True,\n    batch_size=16,\n    collate_fn=data_collator,\n)\ntf_test_set = model.prepare_tf_dataset(\n    tokenized_data[\"test\"],\n    shuffle=False,\n    batch_size=16,\n    collate_fn=data_collator,\n)","metadata":{"id":"076839d6","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:51:44.501060Z","iopub.execute_input":"2024-12-10T03:51:44.501806Z","iopub.status.idle":"2024-12-10T03:51:44.841217Z","shell.execute_reply.started":"2024-12-10T03:51:44.501767Z","shell.execute_reply":"2024-12-10T03:51:44.840528Z"}},"outputs":[],"execution_count":84},{"id":"541fe8a7","cell_type":"code","source":"epochs = 1\nmodel.compile(optimizer=optimizer)","metadata":{"id":"541fe8a7","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:51:47.001052Z","iopub.execute_input":"2024-12-10T03:51:47.001948Z","iopub.status.idle":"2024-12-10T03:51:47.013063Z","shell.execute_reply.started":"2024-12-10T03:51:47.001893Z","shell.execute_reply":"2024-12-10T03:51:47.012203Z"}},"outputs":[],"execution_count":85},{"id":"b7674ab7-48bc-43b3-9dc4-b0d70ebd6fb6","cell_type":"code","source":"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=epochs, callbacks=None)","metadata":{"id":"b7674ab7-48bc-43b3-9dc4-b0d70ebd6fb6","outputId":"a8d8d8ac-351f-4896-85be-cc16f85a79f6","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:51:48.654832Z","iopub.execute_input":"2024-12-10T03:51:48.655165Z","iopub.status.idle":"2024-12-10T03:53:38.408603Z","shell.execute_reply.started":"2024-12-10T03:51:48.655136Z","shell.execute_reply":"2024-12-10T03:53:38.407693Z"}},"outputs":[{"name":"stdout","text":"656/656 [==============================] - 110s 100ms/step - loss: 2.3753 - val_loss: 1.6760\n","output_type":"stream"},{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"<tf_keras.src.callbacks.History at 0x7d414d13ccd0>"},"metadata":{}}],"execution_count":86},{"id":"e8fa8e30","cell_type":"code","source":"# Guarda el modelo entrenado\nfolder_path = '/kaggle/working/model/'\nmodel_name = \"NMT-2024-04-11-epocs-\" + str(epochs)\npath = os.path.join(folder_path, model_name + \".h5\")\nmodel.save_pretrained(path)\ndel model","metadata":{"id":"e8fa8e30","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:53:42.118427Z","iopub.execute_input":"2024-12-10T03:53:42.118790Z","iopub.status.idle":"2024-12-10T03:53:43.489030Z","shell.execute_reply.started":"2024-12-10T03:53:42.118756Z","shell.execute_reply":"2024-12-10T03:53:43.487983Z"}},"outputs":[],"execution_count":87},{"id":"6f69e21f","cell_type":"code","source":"#Para inferir desde aquí.\nmodel_name = \"NMT-2024-04-11-epocs-\" + str(epochs)\npath = os.path.join(folder_path, model_name + \".h5\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\nmodel = TFAutoModelForSeq2SeqLM.from_pretrained(path, pad_token_id=tokenizer.eos_token_id)\n\nsummarizer = pipeline(\"summarization\",\n    model=model,\n    tokenizer=tokenizer,\n    framework=\"tf\")","metadata":{"id":"6f69e21f","outputId":"7931db89-95b9-49b4-c4c5-55c9320dd2e6","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:53:48.043953Z","iopub.execute_input":"2024-12-10T03:53:48.044327Z","iopub.status.idle":"2024-12-10T03:53:49.450216Z","shell.execute_reply.started":"2024-12-10T03:53:48.044298Z","shell.execute_reply":"2024-12-10T03:53:49.449501Z"}},"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n\nAll the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at /kaggle/working/model/NMT-2024-04-11-epocs-1.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}],"execution_count":88},{"id":"6e53ce94-7eb4-41c3-b96e-02456561d172","cell_type":"markdown","source":"# Summarize","metadata":{}},{"id":"062ae818-911c-4e59-b3e1-7b7312bbf385","cell_type":"markdown","source":"## Al usar 5000 ejemplos del dataset","metadata":{}},{"id":"7c40e509-da7f-4b55-b4de-6fb9934b0b06","cell_type":"code","source":"import timeit\nstart_time = timeit.default_timer()\n\ntext = \"summarize: Google is a technology company\"\nprint(summarizer(text, min_length=8, max_length=12))\n\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:37:58.108792Z","iopub.execute_input":"2024-12-10T03:37:58.109136Z","iopub.status.idle":"2024-12-10T03:38:00.857435Z","shell.execute_reply.started":"2024-12-10T03:37:58.109107Z","shell.execute_reply":"2024-12-10T03:38:00.856566Z"}},"outputs":[{"name":"stderr","text":"Your max_length is set to 12, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'Google estoy una tecnologia'}]\n2.744006808999984\n","output_type":"stream"}],"execution_count":45},{"id":"d8d5cc1e-c7e9-47dd-93ae-f0a8052692fa","cell_type":"markdown","source":"## Al usar 10000 ejemplos del dataset","metadata":{}},{"id":"d7c3925f","cell_type":"code","source":"import timeit\nstart_time = timeit.default_timer()\n\ntext = \"summarize: Google is a technology company\"\nprint(summarizer(text, min_length=8, max_length=12))\n\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","metadata":{"id":"d7c3925f","outputId":"3c450f68-3077-4fa1-c857-b3376155b6fb","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:33:40.189387Z","iopub.execute_input":"2024-12-10T03:33:40.190119Z","iopub.status.idle":"2024-12-10T03:33:43.541000Z","shell.execute_reply.started":"2024-12-10T03:33:40.190079Z","shell.execute_reply":"2024-12-10T03:33:43.539994Z"}},"outputs":[{"name":"stderr","text":"Your max_length is set to 12, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'Google está una tecnologia.'}]\n3.3463849889999437\n","output_type":"stream"}],"execution_count":29},{"id":"f5f87239-465a-4e8f-94b4-7020e1b0c00a","cell_type":"markdown","source":"## Al usar 14000 ejemplos del dataset","metadata":{}},{"id":"afac265e-1663-4510-b47d-96abd578b1ed","cell_type":"code","source":"import timeit\nstart_time = timeit.default_timer()\n\ntext = \"summarize: Google is a technology company\"\nprint(summarizer(text, min_length=8, max_length=12))\n\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:40:53.813011Z","iopub.execute_input":"2024-12-10T03:40:53.813382Z","iopub.status.idle":"2024-12-10T03:40:56.624715Z","shell.execute_reply.started":"2024-12-10T03:40:53.813347Z","shell.execute_reply":"2024-12-10T03:40:56.623782Z"}},"outputs":[{"name":"stderr","text":"Your max_length is set to 12, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'Google es una sociedad '}]\n2.8068187880001005\n","output_type":"stream"}],"execution_count":55},{"id":"e3a53733-dbb9-4ea2-ae37-1e3a67d6b841","cell_type":"markdown","source":"# Translate","metadata":{}},{"id":"903438b5-d99e-48a0-958c-c08f25794d51","cell_type":"markdown","source":"## Usando 5000 datos","metadata":{}},{"id":"c4e492dc-76fc-4928-8f02-828c3336a03b","cell_type":"code","source":"import timeit\nstart_time = timeit.default_timer()\n\ntext = \"translate: Google is a technology company\"\nprint(summarizer(text, min_length=8, max_length=20))\n\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:44:10.903866Z","iopub.execute_input":"2024-12-10T03:44:10.904787Z","iopub.status.idle":"2024-12-10T03:44:15.647120Z","shell.execute_reply.started":"2024-12-10T03:44:10.904719Z","shell.execute_reply":"2024-12-10T03:44:15.646158Z"}},"outputs":[{"name":"stderr","text":"Your max_length is set to 20, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'google is a tech company .'}]\n4.7378049489998375\n","output_type":"stream"}],"execution_count":65},{"id":"2ad37c6e-0150-4949-b684-0a71bbbaa66e","cell_type":"markdown","source":"## Usando 10000 datos","metadata":{}},{"id":"a659a0e3-f97c-45d6-afd0-894d889bc8dd","cell_type":"code","source":"import timeit\nstart_time = timeit.default_timer()\n\ntext = \"translate: Google is a technology company\"\nprint(summarizer(text, min_length=8, max_length=12))\n\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:50:52.316682Z","iopub.execute_input":"2024-12-10T03:50:52.317026Z","iopub.status.idle":"2024-12-10T03:50:54.592380Z","shell.execute_reply.started":"2024-12-10T03:50:52.316997Z","shell.execute_reply":"2024-12-10T03:50:54.591446Z"}},"outputs":[{"name":"stderr","text":"Your max_length is set to 12, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'google is a technology company .'}]\n2.2709707560002244\n","output_type":"stream"}],"execution_count":79},{"id":"2ead3274-ee59-4dea-ad27-3402f8fbf0d8","cell_type":"markdown","source":"## Usando 15000 datos","metadata":{}},{"id":"7471a435-a265-4a00-9433-9b1eb5227e7b","cell_type":"code","source":"import timeit\nstart_time = timeit.default_timer()\n\ntext = \"translate: Google is a technology company\"\nprint(summarizer(text, min_length=8, max_length=12))\n\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:54:05.597796Z","iopub.execute_input":"2024-12-10T03:54:05.598150Z","iopub.status.idle":"2024-12-10T03:54:08.407630Z","shell.execute_reply.started":"2024-12-10T03:54:05.598117Z","shell.execute_reply":"2024-12-10T03:54:08.406731Z"}},"outputs":[{"name":"stderr","text":"Your max_length is set to 12, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'google is a tech company .'}]\n2.805183269999816\n","output_type":"stream"}],"execution_count":89},{"id":"f6907067-93f6-4258-9150-7fa14cbbadca","cell_type":"markdown","source":"# Resultados\n\nAl usar una mayor cantidad de datos los resultados observados llegan a ser mejores, por lo cuál con un dataset más grande, se podría crear un LLM robusto y util para traducir y resumir frases de más de 12 o 20 caracteres.\nEn la tarea de traducir no logra el resultado esperado, por lo que tal vez usando más datos pueda llegar a hacerlo de una manera más satisfactoria.","metadata":{}}]}